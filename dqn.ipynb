{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import tqdm\n",
    "import random\n",
    "import gym\n",
    "import gym_multi_car_racing\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from racetrack_env import RaceTrack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customized weight initialization\n",
    "def customized_weights_init(m):\n",
    "    # compute the gain\n",
    "    gain = nn.init.calculate_gain('relu')\n",
    "    # init the convolutional layer\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        # init the params using uniform\n",
    "        nn.init.xavier_uniform_(m.weight, gain=gain)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    # init the linear layer\n",
    "    if isinstance(m, nn.Linear):\n",
    "        # init the params using uniform\n",
    "        nn.init.xavier_uniform_(m.weight, gain=gain)\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNet(nn.Module):\n",
    "    def __init__(self, input_dim, num_hidden_layer, dim_hidden_layer, output_dim):\n",
    "        super(DeepQNet, self).__init__()\n",
    "\n",
    "        \"\"\"CODE HERE: construct your Deep neural network\n",
    "        \"\"\"\n",
    "         # define the input dimension\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        # define the hidden dimension\n",
    "        self.hidden_num = num_hidden_layer\n",
    "\n",
    "        # define the number of the hidden layers\n",
    "        self.hidden_dim = dim_hidden_layer\n",
    "\n",
    "        # define the output dimension\n",
    "        self.output_dim = output_dim\n",
    "      \n",
    "        self.dqn=nn.Sequential(nn.Linear(self.input_dim,self.hidden_dim),nn.ReLU(),nn.Linear(self.hidden_dim,self.hidden_dim),nn.ReLU(),\n",
    "                                                                   nn.Linear(self.hidden_dim,self.hidden_dim),nn.ReLU(),nn.Linear(self.hidden_dim,self.output_dim))\n",
    "                               \n",
    "\n",
    "    def forward(self,x):\n",
    "#         print(x.shape)\n",
    "        y=self.dqn(x)\n",
    "        return y\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\" Implement the Replay Buffer as a class, which contains:\n",
    "            - self._data_buffer (list): a list variable to store all transition tuples.\n",
    "            - add: a function to add new transition tuple into the buffer\n",
    "            - sample_batch: a function to sample a batch training data from the Replay Buffer\n",
    "    \"\"\"\n",
    "    def __init__(self, buffer_size):\n",
    "        \"\"\"Args:\n",
    "               buffer_size (int): size of the replay buffer\n",
    "        \"\"\"\n",
    "        # total size of the replay buffer\n",
    "        self.total_size = buffer_size\n",
    "\n",
    "        # create a list to store the transitions\n",
    "        self._data_buffer = []\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data_buffer)\n",
    "\n",
    "    def add(self, obs, act, reward, next_obs, done):\n",
    "        # create a tuple\n",
    "        trans = (obs, act, reward, next_obs, done)\n",
    "\n",
    "        # interesting implementation\n",
    "        if self._next_idx >= len(self._data_buffer):\n",
    "            self._data_buffer.append(trans)\n",
    "        else:\n",
    "            self._data_buffer[self._next_idx] = trans\n",
    "\n",
    "        # increase the index\n",
    "        self._next_idx = (self._next_idx + 1) % self.total_size\n",
    "\n",
    "    def _encode_sample(self, indices):\n",
    "        \"\"\" Function to fetch the state, action, reward, next state, and done arrays.\n",
    "        \n",
    "            Args:\n",
    "                indices (list): list contains the index of all sampled transition tuples.\n",
    "        \"\"\"\n",
    "        # lists for transitions\n",
    "        obs_list, actions_list, rewards_list, next_obs_list, dones_list = [], [], [], [], []\n",
    "\n",
    "        # collect the data\n",
    "        for idx in indices:\n",
    "            # get the single transition\n",
    "            data = self._data_buffer[idx]\n",
    "            obs, act, reward, next_obs, d = data\n",
    "            # store to the list\n",
    "            obs_list.append(np.array(obs, copy=False))\n",
    "            actions_list.append(np.array(act, copy=False))\n",
    "            rewards_list.append(np.array(reward, copy=False))\n",
    "            next_obs_list.append(np.array(next_obs, copy=False))\n",
    "            dones_list.append(np.array(d, copy=False))\n",
    "        # return the sampled batch data as numpy arrays\n",
    "        return np.array(obs_list), np.array(actions_list), np.array(rewards_list), np.array(next_obs_list), np.array(\n",
    "            dones_list)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        \"\"\" Args:\n",
    "                batch_size (int): size of the sampled batch data.\n",
    "        \"\"\"\n",
    "        # sample indices with replaced\n",
    "        indices = [np.random.randint(0, len(self._data_buffer)) for _ in range(batch_size)]\n",
    "        return self._encode_sample(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSchedule(object):\n",
    "    \"\"\" This schedule returns the value linearly\"\"\"\n",
    "    def __init__(self, start_value, end_value, duration):\n",
    "        # start value\n",
    "        self._start_value = start_value\n",
    "        # end value\n",
    "        self._end_value = end_value\n",
    "        # time steps that value changes from the start value to the end value\n",
    "        self._duration = duration\n",
    "        # difference between the start value and the end value\n",
    "        self._schedule_amount = end_value - start_value\n",
    "        \n",
    "\n",
    "    def get_value(self, time):\n",
    "        # logic: if time > duration, use the end value, else use the scheduled value\n",
    "        \"\"\" CODE HERE: return the epsilon for each time step within the duration.\n",
    "        \"\"\"\n",
    "        if time>self._duration:\n",
    "            return self._end_value\n",
    "        else:\n",
    "            return self._start_value + self._schedule_amount * time / self._duration\n",
    "\n",
    "      \n",
    "        \n",
    "        \n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    # initialize the agent\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 ):\n",
    "        # save the parameters\n",
    "        self.params = params\n",
    "\n",
    "        # environment parameters\n",
    "        self.action_dim = params['action_dim']\n",
    "        self.obs_dim = params['observation_dim']\n",
    "       \n",
    "\n",
    "        # executable actions\n",
    "        self.action_space = params['action_space']\n",
    "\n",
    "        # create value network\n",
    "        self.behavior_policy_net = DeepQNet(input_dim=params['observation_dim'],\n",
    "                                   num_hidden_layer=params['hidden_layer_num'],\n",
    "                                   dim_hidden_layer=params['hidden_layer_dim'],\n",
    "                                   output_dim=params['action_dim'])\n",
    "        # create target network\n",
    "        self.target_policy_net = DeepQNet(input_dim=params['observation_dim'],\n",
    "                                          num_hidden_layer=params['hidden_layer_num'],\n",
    "                                          dim_hidden_layer=params['hidden_layer_dim'],\n",
    "                                          output_dim=params['action_dim'])\n",
    "\n",
    "        # initialize target network with behavior network\n",
    "        self.behavior_policy_net.apply(customized_weights_init)\n",
    "        self.target_policy_net.load_state_dict(self.behavior_policy_net.state_dict())\n",
    "\n",
    "        # send the agent to a specific device: cpu or gpu\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.behavior_policy_net.to(self.device)\n",
    "        self.target_policy_net.to(self.device)\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.behavior_policy_net.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    # get action\n",
    "    def get_action(self, obs, eps):\n",
    "        if np.random.random() < eps:  # with probability eps, the agent selects a random action\n",
    "            action = np.random.choice(self.action_space, 1)[0]\n",
    "            return action\n",
    "        else:  # with probability 1 - eps, the agent selects a greedy policy\n",
    "            obs = self._arr_to_tensor(obs).view(1, -1)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.behavior_policy_net(obs)\n",
    "                action = q_values.max(dim=1)[1].item()\n",
    "            return self.action_space[int(action)]\n",
    "\n",
    "    # update behavior policy\n",
    "    def update_behavior_policy(self, batch_data):\n",
    "        # convert batch data to tensor and put them on device\n",
    "        batch_data_tensor = self._batch_to_tensor(batch_data)\n",
    "\n",
    "        # get the transition data\n",
    "        obs_tensor = batch_data_tensor['obs']\n",
    "        actions_tensor = batch_data_tensor['action']\n",
    "        next_obs_tensor = batch_data_tensor['next_obs']\n",
    "        rewards_tensor = batch_data_tensor['reward']\n",
    "        dones_tensor = batch_data_tensor['done']\n",
    "\n",
    "        \"\"\"CODE HERE:\n",
    "                Compute the predicted Q values using the behavior policy network\n",
    "        \"\"\"\n",
    "        # compute the q value estimation using the behavior network\n",
    "        q_current=self.behavior_policy_net(obs_tensor).gather(1,actions_tensor)\n",
    "        # next_state_values=torch.zeros(self.params['batch_size'],device=self.device)\n",
    "        next_state_values=self.target_policy_net(next_obs_tensor).max(1)[0].view(-1,1)\n",
    "       \n",
    "        # compute the TD target using the target network\n",
    "        TD_target=rewards_tensor+self.params['gamma']*next_state_values*(1-dones_tensor)\n",
    "      \n",
    "        # compute the loss\n",
    "        td_loss=torch.nn.functional.mse_loss(q_current,TD_target)\n",
    "   \n",
    "        # minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        td_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return td_loss.item()\n",
    "\n",
    "    # update target policy\n",
    "    def update_target_policy(self):\n",
    "        # hard update\n",
    "        \"\"\"CODE HERE: \n",
    "                Copy the behavior policy network to the target network\n",
    "        \"\"\"\n",
    "        self.target_policy_net.load_state_dict(self.behavior_policy_net.state_dict())\n",
    "\n",
    "    # auxiliary functions\n",
    "    def _arr_to_tensor(self, arr):\n",
    "        arr = np.array(arr)\n",
    "        arr_tensor = torch.from_numpy(arr).float().to(self.device)\n",
    "        return arr_tensor\n",
    "\n",
    "    def _batch_to_tensor(self, batch_data):\n",
    "        # store the tensor\n",
    "        batch_data_tensor = {'obs': [], 'action': [], 'reward': [], 'next_obs': [], 'done': []}\n",
    "        # get the numpy arrays\n",
    "        obs_arr, action_arr, reward_arr, next_obs_arr, done_arr = batch_data\n",
    "        # convert to tensors\n",
    "        batch_data_tensor['obs'] = torch.tensor(obs_arr, dtype=torch.float32).to(self.device)\n",
    "        batch_data_tensor['action'] = torch.tensor(action_arr).long().view(-1, 1).to(self.device)\n",
    "        batch_data_tensor['reward'] = torch.tensor(reward_arr, dtype=torch.float32).view(-1, 1).to(self.device)\n",
    "        batch_data_tensor['next_obs'] = torch.tensor(next_obs_arr, dtype=torch.float32).to(self.device)\n",
    "        batch_data_tensor['done'] = torch.tensor(done_arr, dtype=torch.float32).view(-1, 1).to(self.device)\n",
    "\n",
    "        return batch_data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn_agent(env, params):\n",
    "    # create the DQN agent\n",
    "    my_agent = DQNAgent(params)\n",
    "\n",
    "    # create the epsilon-greedy schedule\n",
    "    my_schedule = LinearSchedule(start_value=params['epsilon_start_value'],\n",
    "                                 end_value=params['epsilon_end_value'],\n",
    "                                 duration=params['epsilon_duration'])\n",
    "\n",
    "    # create the replay buffer\n",
    "    replay_buffer = ReplayBuffer(params['replay_buffer_size'])\n",
    "\n",
    "    # training variables\n",
    "    episode_t = 0\n",
    "    rewards = []\n",
    "    train_returns = []\n",
    "    train_loss = []\n",
    "    loss = 0\n",
    "\n",
    "    # reset the environment\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    # start training\n",
    "    pbar = tqdm.trange(params['total_training_time_step'])\n",
    "    last_best_return = 0\n",
    "    for t in pbar:\n",
    "        # scheduled epsilon at time step t\n",
    "        eps_t = my_schedule.get_value(t)\n",
    "      \n",
    "        # get one epsilon-greedy action\n",
    "        action = my_agent.get_action(obs, eps_t)\n",
    "\n",
    "        # step in the environment\n",
    "        next_obs, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        # add to the buffer\n",
    "        replay_buffer.add(obs, env.action_names.index(action), reward, next_obs, done)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        # check termination\n",
    "        if done:\n",
    "            # compute the return\n",
    "            G = 0\n",
    "            for r in reversed(rewards):\n",
    "                G = r + params['gamma'] * G\n",
    "\n",
    "            if G > last_best_return:\n",
    "                torch.save(my_agent.behavior_policy_net.state_dict(), f\"./{params['model_name']}\")\n",
    "\n",
    "            # store the return\n",
    "            train_returns.append(-1*G)\n",
    "            episode_idx = len(train_returns)\n",
    "\n",
    "            # print the information\n",
    "            pbar.set_description(\n",
    "                f\"Ep={episode_idx} | \"\n",
    "                f\"G={np.mean(train_returns[-10:]) if train_returns else 0:.2f} | \"\n",
    "                f\"Eps={eps_t}\"\n",
    "            )\n",
    "\n",
    "            # reset the environment\n",
    "            episode_t, rewards = 0, []\n",
    "            obs, _ = env.reset()\n",
    "        else:\n",
    "            # increment\n",
    "            obs = next_obs\n",
    "            episode_t += 1\n",
    "\n",
    "        if t > params['start_training_step']:\n",
    "            # update the behavior model\n",
    "            if not np.mod(t, params['freq_update_behavior_policy']):\n",
    "                \"\"\" CODE HERE:\n",
    "                    Update the behavior policy network\n",
    "                \"\"\"\n",
    "                batch_data=replay_buffer.sample_batch(params['batch_size'])\n",
    "                \n",
    "                train_loss.append(my_agent.update_behavior_policy(batch_data))\n",
    "            # update the target model\n",
    "            if not np.mod(t, params['freq_update_target_policy']):\n",
    "                \"\"\" CODE HERE:\n",
    "                    Update the behavior policy network\n",
    "                \"\"\"\n",
    "                my_agent.update_target_policy()\n",
    "              \n",
    "    # save the results\n",
    "    return train_returns, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(arr_list, legend_list, color_list, ylabel, fig_title):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        arr_list (list): list of results arrays to plot\n",
    "        legend_list (list): list of legends corresponding to each result array\n",
    "        color_list (list): list of color corresponding to each result array\n",
    "        ylabel (string): label of the Y axis\n",
    "\n",
    "        Note that, make sure the elements in the arr_list, legend_list and color_list are associated with each other correctly.\n",
    "        Do not forget to change the ylabel for different plots.\n",
    "    \"\"\"\n",
    "    # set the figure type\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # PLEASE NOTE: Change the labels for different plots\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(\"Time Steps\")\n",
    "\n",
    "    # ploth results\n",
    "    h_list = []\n",
    "    for arr, legend, color in zip(arr_list, legend_list, color_list):\n",
    "        # compute the standard error\n",
    "        arr_err = arr.std(axis=0) / np.sqrt(arr.shape[0])\n",
    "        # plot the mean\n",
    "        h, = ax.plot(range(arr.shape[1]), arr.mean(axis=0), color=color, label=legend)\n",
    "        # plot the confidence band\n",
    "        arr_err *= 1.96\n",
    "        ax.fill_between(range(arr.shape[1]), arr.mean(axis=0) - arr_err, arr.mean(axis=0) + arr_err, alpha=0.3,\n",
    "                        color=color)\n",
    "        # save the plot handle\n",
    "        h_list.append(h)\n",
    "\n",
    "    # plot legends\n",
    "    ax.set_title(f\"{fig_title}\")\n",
    "    ax.legend(handles=h_list)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]/home/amina/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:197: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Ep=10 | G=8974.08 | Eps=0.01: 100%|██████████| 10000/10000 [00:03<00:00, 2542.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [6818.129252627615, 9299.493421240366, 9195.274085479643, 9398.823957436545, 9390.378743390855, 9348.10357620361, 9067.355642425839, 8819.713214422893, 9112.982324064771, 9290.50847338798]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep=10 | G=8928.47 | Eps=0.01: 100%|██████████| 10000/10000 [00:04<00:00, 2414.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [6818.129252627615, 9299.493421240366, 9195.274085479643, 9398.823957436545, 9390.378743390855, 9348.10357620361, 9067.355642425839, 8819.713214422893, 9112.982324064771, 9290.50847338798], 1: [6877.134812896487, 9131.28530636593, 8960.219671750294, 8970.62173210587, 9336.116548673606, 9288.62806207007, 9256.278989366105, 9288.317894147458, 9012.85334000455, 9163.228963259175]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep=10 | G=9050.90 | Eps=0.01: 100%|██████████| 10000/10000 [00:04<00:00, 2499.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [6818.129252627615, 9299.493421240366, 9195.274085479643, 9398.823957436545, 9390.378743390855, 9348.10357620361, 9067.355642425839, 8819.713214422893, 9112.982324064771, 9290.50847338798], 1: [6877.134812896487, 9131.28530636593, 8960.219671750294, 8970.62173210587, 9336.116548673606, 9288.62806207007, 9256.278989366105, 9288.317894147458, 9012.85334000455, 9163.228963259175], 2: [7309.671582687745, 9291.967037199693, 9269.766367919501, 9132.139744638689, 9347.731511149677, 9281.514432353213, 9400.601249058813, 9431.411488600776, 9076.106092153526, 8968.128533889994]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep=10 | G=8982.67 | Eps=0.01: 100%|██████████| 10000/10000 [00:03<00:00, 2572.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [6818.129252627615, 9299.493421240366, 9195.274085479643, 9398.823957436545, 9390.378743390855, 9348.10357620361, 9067.355642425839, 8819.713214422893, 9112.982324064771, 9290.50847338798], 1: [6877.134812896487, 9131.28530636593, 8960.219671750294, 8970.62173210587, 9336.116548673606, 9288.62806207007, 9256.278989366105, 9288.317894147458, 9012.85334000455, 9163.228963259175], 2: [7309.671582687745, 9291.967037199693, 9269.766367919501, 9132.139744638689, 9347.731511149677, 9281.514432353213, 9400.601249058813, 9431.411488600776, 9076.106092153526, 8968.128533889994], 3: [7062.925438189897, 9311.077289117373, 9155.893757740114, 9237.55602110277, 8969.812446763694, 9181.937898377084, 9295.190256502518, 9226.076790298303, 9181.37890826938, 9204.890262797886]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep=10 | G=9014.23 | Eps=0.01: 100%|██████████| 10000/10000 [00:03<00:00, 2577.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [6818.129252627615, 9299.493421240366, 9195.274085479643, 9398.823957436545, 9390.378743390855, 9348.10357620361, 9067.355642425839, 8819.713214422893, 9112.982324064771, 9290.50847338798], 1: [6877.134812896487, 9131.28530636593, 8960.219671750294, 8970.62173210587, 9336.116548673606, 9288.62806207007, 9256.278989366105, 9288.317894147458, 9012.85334000455, 9163.228963259175], 2: [7309.671582687745, 9291.967037199693, 9269.766367919501, 9132.139744638689, 9347.731511149677, 9281.514432353213, 9400.601249058813, 9431.411488600776, 9076.106092153526, 8968.128533889994], 3: [7062.925438189897, 9311.077289117373, 9155.893757740114, 9237.55602110277, 8969.812446763694, 9181.937898377084, 9295.190256502518, 9226.076790298303, 9181.37890826938, 9204.890262797886], 4: [6909.093078582696, 9098.285732950324, 9453.315451481747, 9141.055193378435, 9240.620391732287, 9247.585314227052, 9325.79002133017, 9075.511527585146, 9282.676034240434, 9368.406492508726]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # set the random seed\n",
    "    np.random.seed(1234)\n",
    "    random.seed(1234)\n",
    "    torch.manual_seed(1234)\n",
    "\n",
    "    # create environment\n",
    "    my_env = RaceTrack(track_file_path=\"tracks/simple.txt\", \n",
    "                starting_position=(1, 2), \n",
    "                goal_positions=[\n",
    "                    (1, 6), (2, 6), (3, 6),\n",
    "                    (1, 5), (2, 5), (3, 5),\n",
    "                ],\n",
    "                eye_sight=2,model_type='numerical')\n",
    "\n",
    "    # create training parameters\n",
    "    train_parameters = {\n",
    "        'observation_dim': 25,\n",
    "        'action_dim': 5,\n",
    "        'action_space': my_env.action_names,\n",
    "        'hidden_layer_num': 2,\n",
    "        'hidden_layer_dim': 128,\n",
    "        'gamma': 0.99995,\n",
    "\n",
    "        'total_training_time_step': 10000,\n",
    "\n",
    "        'epsilon_start_value': 1.0,\n",
    "        'epsilon_end_value': 0.01,\n",
    "        'epsilon_duration': 250,\n",
    "\n",
    "        'replay_buffer_size': 100000,\n",
    "        'start_training_step': 50,\n",
    "        'freq_update_behavior_policy': 40,\n",
    "        'freq_update_target_policy': 200,\n",
    "\n",
    "        'batch_size': 32,\n",
    "        'learning_rate': 1e-3,\n",
    "\n",
    "        'model_name': \"Racetrack.pt\"\n",
    "    }\n",
    "\n",
    "    # create experiment\n",
    "    run_trial = 5\n",
    "    train_returns={}\n",
    "    train_loss={}\n",
    "    for i in range(run_trial):\n",
    "        train_returns[i], train_loss[i] = train_dqn_agent(my_env, train_parameters)\n",
    "        print(train_returns)\n",
    "        min_value=min([len(return_value) for return_value in train_returns.values()])\n",
    "\n",
    "    train_returns_clip=[]\n",
    "    for trial_num,returns in train_returns.items():\n",
    "        train_returns_clip.append(returns[:min_value])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
